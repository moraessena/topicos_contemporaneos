{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-07 18:53:19--  https://pjreddie.com/media/files/mnist_train.csv\n",
      "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
      "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 109575994 (104M) [text/csv]\n",
      "Saving to: ‘data/mnist_train.csv.1’\n",
      "\n",
      "mnist_train.csv.1   100%[===================>] 104,50M  3,36MB/s    in 30s     \n",
      "\n",
      "2024-08-07 18:53:54 (3,45 MB/s) - ‘data/mnist_train.csv.1’ saved [109575994/109575994]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download do dataset MNIST into data dir\n",
    "! wget https://pjreddie.com/media/files/mnist_train.csv -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # A primeira coluna é o rótulo\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        # As demais colunas são os pixels da imagem\n",
    "        image = self.data.iloc[idx, 1:].values.astype('uint8').reshape(28, 28)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação para converter a imagem para tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),      # Convertendo a imagem PIL para tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizando a imagem\n",
    "])\n",
    "\n",
    "# Criando o dataset\n",
    "mnist_dataset = MNISTDataset(\"./data/mnist_train.csv\", transform=transform)\n",
    "\n",
    "# Transformação para converter a imagem para tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),    # Convertendo o array numpy para imagem PIL\n",
    "    transforms.ToTensor(),      # Convertendo a imagem PIL para tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizando a imagem\n",
    "])\n",
    "\n",
    "# Criando o dataset\n",
    "mnist_dataset = MNISTDataset(\"./data/mnist_train.csv\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando índices aleatórios para os subsets de treino e teste\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(mnist_dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Dividindo os índices em trainset e testset\n",
    "num_train = 1000\n",
    "num_val = 500\n",
    "num_test = 500\n",
    "train_indices = indices[:num_train]  # Primeiros 1.000 índices para o trainset\n",
    "val_indices = indices[num_train:num_train+num_val]  # Próximos 500 índices para o valset\n",
    "test_indices = indices[num_train+num_val:num_train+num_val+num_test]  # Mais 500 para o testset\n",
    "\n",
    "trainset = Subset(mnist_dataset, train_indices)\n",
    "valset = Subset(mnist_dataset, val_indices)\n",
    "testset = Subset(mnist_dataset, test_indices)\n",
    "\n",
    "# Criando DataLoaders para os subsets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, valloader, criterion, optimizer, num_epochs=5):\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_accuracies': [],\n",
    "        'val_accuracies': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct / total\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['train_accuracies'].append(train_acc)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.2f}%')\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['val_accuracies'].append(val_acc)\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {val_loss:.3f}, Val Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "    print('Treinamento concluído')\n",
    "    return history\n",
    "\n",
    "\n",
    "def test_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Acurácia da rede na base de teste: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, initialize_weights=True, dropout=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        self.fc1 = nn.Linear(12 * 4 * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if initialize_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(-1, 12 * 4 * 4)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    initialize_weights=True,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando a CNN\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "# Plot de losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.grid()\n",
    "plt.plot(epochs, history['train_losses'], label='Train Loss')\n",
    "plt.plot(epochs, history['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot de accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.grid()\n",
    "plt.plot(epochs, history['train_accuracies'], label='Train Accuracy')\n",
    "plt.plot(epochs, history['val_accuracies'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Na CNN implementada, execute experimentos variando:\n",
    "- O número de filtros convolucionais em `self.conv1` (6 e 8)\n",
    "- O número de filtros convolucionais em `self.conv2` (12 e 16)\n",
    "\n",
    "Ao final, observe qual combinação desempenha melhor no conjunto de testes.\n",
    "\n",
    "Dica: Adicionar argumentos no construtor da classe para determinar o número de filtros em cada camada pode ser útil para inicializar diversos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "\n",
    "Na CNN implementada, execute 5 treinamentos variando aleatoriamente o número de neurônios de saída em `self.fc1` (o valor atual é 64 e também deve ser alterado na entrada da próxima camada).\n",
    "\n",
    "Como você determinaria qual foi o melhor valor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     num_neurons = np.random.randint(10, 100)\n",
    "#     model = ...\n",
    "#     criterion = ...\n",
    "#     optimizer = ...\n",
    "#     history = ...\n",
    "\n",
    "#     print(f'Número de neurônios na camada fully connected: {num_neurons}')\n",
    "#     test_model(model, testloader)\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
