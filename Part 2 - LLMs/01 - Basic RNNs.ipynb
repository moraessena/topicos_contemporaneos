{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Redes Neurais Recorrentes (RNNs)**\n",
    "\n",
    "As Redes Neurais Recorrentes (RNNs) são um tipo de rede neural projetada para processar sequências de dados, como séries temporais ou textos. A principal característica das RNNs é a capacidade de manter um estado interno que captura informações sobre as entradas anteriores, permitindo que a rede tenha \"memória\" dos passos anteriores na sequência.\n",
    "\n",
    "A cada passo de tempo $t$, a RNN atualiza seu estado oculto $h_t$ com base na entrada atual $x_t$ e no estado oculto anterior $h_{t-1}$:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{ih} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Aqui:\n",
    "- $W_{ih}$ é a matriz de pesos que conecta a entrada ao estado oculto.\n",
    "- $W_{hh}$ é a matriz de pesos que conecta o estado oculto anterior ao atual.\n",
    "- $b_h$ é o termo de bias.\n",
    "- $\\tanh$ é a função de ativação que introduz não-linearidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementação de uma Célula RNN**\n",
    "\n",
    "Nesta seção, implementaremos uma célula RNN personalizada usando o PyTorch. A célula RNN é a unidade básica de uma RNN, responsável por calcular o novo estado oculto com base na entrada atual e no estado oculto anterior.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "O código a seguir define uma célula RNN personalizada:\n",
    "\n",
    "- **Entrada:** O tamanho da entrada `input_size` e o tamanho do estado oculto `hidden_size`.\n",
    "- **Pesos:** $ W_{ih} $ e $ W_{hh} $ são as matrizes de pesos, e $ b_h $ é o bias.\n",
    "- **Saída:** A célula computa o novo estado oculto $ h_t $ a partir da entrada e do estado oculto anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.W_ih = nn.Linear(input_size, hidden_size, bias=False) # Define a matriz de pesos de entrada\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=False) # Define a matriz de pesos de saída\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size)) # Define o bias\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # x: (batch_size, input_size)\n",
    "        # h: (batch_size, hidden_size)\n",
    "        ht = torch.tanh(self.W_ih(x) + self.W_hh(h) + self.b_h)\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exemplo de Uso da Célula RNN**\n",
    "\n",
    "Inicialmente, definimos uma célula RNN com um tamanho de entrada de 10 e um tamanho de estado oculto de 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "input_size = 10\n",
    "hidden_size = 4\n",
    "\n",
    "rnn_cell = RNNCell(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inspeção dos Parâmetros da Célula RNN**\n",
    "\n",
    "Neste trecho de código, vamos inspecionar os parâmetros da célula RNN que acabamos de definir. Em uma rede neural, os parâmetros são os valores ajustáveis (como os pesos e bias) que a rede aprende durante o treinamento.\n",
    "\n",
    "Ao executar o código, veremos os nomes e as formas dos parâmetros:\n",
    "\n",
    "- **`W_ih`**: Matriz de pesos conectando a entrada ao estado oculto.\n",
    "- **`W_hh`**: Matriz de pesos conectando o estado oculto anterior ao atual.\n",
    "- **`b_h`**: Termo de bias para o estado oculto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in rnn_cell.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Explicação do Código**\n",
    "\n",
    "No exemplo abaixo:\n",
    "- Inicializamos uma entrada $x$ aleatória e um estado oculto inicial $h$ como zero.\n",
    "- A célula RNN processa a entrada e gera um novo estado oculto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(input_size)\n",
    "h = torch.zeros(hidden_size)\n",
    "\n",
    "hn = rnn_cell(x, h)\n",
    "\n",
    "hn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processamento de Sequências com RNN**\n",
    "\n",
    "RNNs são projetadas para processar sequências de dados, onde cada passo de tempo depende dos passos anteriores. Vamos demonstrar isso processando uma sequência de entradas com a célula RNN.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "Neste exemplo:\n",
    "- Processamos uma sequência de 3 entradas $x$ e atualizamos o estado oculto $h$ a cada passo.\n",
    "- O estado oculto é propagado ao longo da sequência, capturando informações temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(seq_len, input_size)\n",
    "h = torch.zeros(hidden_size)\n",
    "\n",
    "# Para cada instante de tempo\n",
    "for i in range(seq_len):\n",
    "    # Atualiza o estado oculto\n",
    "    h = rnn_cell(x[i], h)\n",
    "    print(f\"Passo {i+1}\")\n",
    "    print(f\"h = {h}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processamento de Sequências em Lote (Batched RNN)**\n",
    "\n",
    "Para aumentar a eficiência, as RNNs podem processar várias sequências simultaneamente, usando o conceito de \"batching\". Cada sequência no lote é processada de forma independente, mas compartilhando os mesmos pesos da RNN.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "Neste exemplo:\n",
    "- Criamos um batch de 8 sequências, cada uma com 10 entradas.\n",
    "- A célula RNN é aplicada ao batch, atualizando o estado oculto para cada sequência de forma paralela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched RNN\n",
    "batch_size = 8\n",
    "\n",
    "x = torch.randn(batch_size, input_size)\n",
    "h = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "hn = rnn_cell(x, h)\n",
    "\n",
    "hn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processamento de Sequências em Lote com Múltiplos Passos de Tempo**\n",
    "\n",
    "Aqui, processamos um batch de sequências ao longo de vários passos de tempo.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "Neste exemplo:\n",
    "- Processamos um lote de 8 sequências, cada uma com 3 passos de tempo.\n",
    "- Em cada passo de tempo, a célula RNN atualiza o estado oculto para todas as sequências do lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "h = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "# Para cada instante de tempo\n",
    "for i in range(seq_len):\n",
    "    # Atualiza o estado oculto\n",
    "    h = rnn_cell(x[:, i], h)\n",
    "    print(f\"Passo {i}\")\n",
    "    print(f\"h = {h.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construção de uma Rede RNN Completa**\n",
    "\n",
    "Vamos expandir a célula RNN que criamos anteriormente para uma rede RNN completa. A rede RNN será composta por uma célula RNN seguida por uma camada linear que produzirá a saída final.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "- **`self.cell`**: Instância da célula RNN que processará cada passo da sequência.\n",
    "- **`self.W_ho`**: Camada linear que mapeia o estado oculto final para a saída desejada.\n",
    "\n",
    "No método `forward`:\n",
    "- **Entrada (`x`)**: Um tensor de forma `(batch_size, seq_len, input_size)`, onde `seq_len` é o comprimento da sequência.\n",
    "- **Estado oculto (`h`)**: Se não for fornecido, é inicializado como um vetor de zeros.\n",
    "- **Processamento da sequência**: A entrada é processada passo a passo pela célula RNN.\n",
    "- **Saída (`y`)**: Calculada pela camada linear a partir do estado oculto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.cell = RNNCell(input_size, hidden_size) # Define a célula RNN\n",
    "        self.W_ho = nn.Linear(hidden_size, output_size) # Define a matriz de pesos de saída\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # h: (batch_size, hidden_size)\n",
    "        if h is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size)\n",
    "        h = h.to(x.device)\n",
    "        \n",
    "        # Processa a sequência\n",
    "        for i in range(seq_len):\n",
    "            h = self.cell(x[:, i], h)\n",
    "\n",
    "        # Calcula a saída\n",
    "        y = self.W_ho(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Geração de Dados para Treinamento**\n",
    "\n",
    "Para treinar a RNN, precisamos de um conjunto de dados sequenciais. Neste exemplo, geramos uma série temporal baseada em uma função senoide com ruído adicionado.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "- **`x`**: Sequência de pontos no intervalo $[0, 6\\pi]$.\n",
    "- **`y`**: Valores da função senoide com ruído gaussiano adicionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "\n",
    "x = np.linspace(0, 6*np.pi, num_points)\n",
    "y = np.sin(x) + np.random.normal(0, 0.3, num_points)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparação das Sequências de Treinamento**\n",
    "\n",
    "Agora, preparamos as entradas e as saídas alvo para o treinamento da RNN. Cada sequência de entrada será usada para prever o próximo ponto na série temporal.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "- **`sequence_length`**: O comprimento da janela da sequência usada como entrada.\n",
    "- **`inputs`**: Sequências de tamanho `sequence_length` extraídas dos dados.\n",
    "- **`targets`**: O ponto subsequente na sequência que a rede deve prever.\n",
    "- **Tensorização**: As sequências são convertidas em tensores PyTorch para uso na rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "batch_size = num_points - sequence_length\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(num_points - sequence_length):\n",
    "    inputs.append(y[i:i+sequence_length])\n",
    "    targets.append(y[i+sequence_length])\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(-1)  # Shape: (batch_size, window_size, 1)\n",
    "targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(-1)  # Shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inicialização da Rede RNN e do Otimizador**\n",
    "\n",
    "Nesta seção, instanciamos a rede RNN com os tamanhos de entrada, estado oculto, e saída definidos. Também configuramos o critério de perda e o otimizador.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "- **`input_size`**: Dimensão da entrada (1, já que estamos lidando com uma série temporal univariada).\n",
    "- **`hidden_size`**: Número de unidades no estado oculto.\n",
    "- **`output_size`**: Dimensão da saída (1, pois queremos prever um único valor por vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = rnn(inputs)\n",
    "    loss = criterion(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Geração de Predições com a Rede Treinada**\n",
    "\n",
    "Após o treinamento, usamos a RNN para gerar previsões de uma nova série temporal. A ideia é prever a continuação da senoide a partir de uma sequência inicial.\n",
    "\n",
    "#### **Explicação do Código**\n",
    "\n",
    "- **`predicted_wave`**: Lista para armazenar as predições feitas pela RNN.\n",
    "- **`input_seq`**: Sequência inicial usada para começar a predição.\n",
    "- **Passos de predição**:\n",
    "  - A cada passo, a rede faz uma nova predição que é adicionada à sequência de entrada.\n",
    "  - A janela de entrada é atualizada para incluir a nova predição e excluir o ponto mais antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando predições para uma senoide\n",
    "predicted_wave = []\n",
    "\n",
    "input_seq = y[:sequence_length].reshape(1, sequence_length, 1)\n",
    "input_seq = torch.tensor(input_seq, dtype=torch.float32)\n",
    "\n",
    "for _ in range(num_points - sequence_length):\n",
    "    predicted_next = rnn(input_seq)\n",
    "    predicted_wave.append(predicted_next.item())\n",
    "\n",
    "    # Arrasta a janela de entrada\n",
    "    predicted_next = predicted_next.view(1, 1, 1)  # Reshape para (1, 1, 1)\n",
    "    input_seq = torch.cat([input_seq[:, 1:, :], predicted_next], dim=1) # Remove o primeiro elemento e adiciona a predição\n",
    "\n",
    "# Plota a senoide original e a predição\n",
    "plt.scatter(x, y, label='Original')\n",
    "plt.scatter(x[sequence_length:], predicted_wave, label='Predito', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercícios**\n",
    "\n",
    "### **Exercício 1: Aumentando o Tamanho do Estado Oculto**\n",
    "\n",
    "1. Aumente o tamanho do estado oculto (`hidden_size`) na rede RNN.\n",
    "2. Treine a rede novamente com os mesmos dados de entrada.\n",
    "3. Compare a perda final e as predições da rede com o modelo original.\n",
    "4. Pergunta: Como o aumento do tamanho do estado oculto afetou o desempenho da rede?\n",
    "\n",
    "### **Exercício 2: Predição de Longo Prazo**\n",
    "\n",
    "1. Use a RNN treinada para prever 100 pontos futuros da senoide a partir de uma nova sequência inicial de 20 pontos.\n",
    "2. Aumente o número de camadas ocultas e tente novamente.\n",
    "2. Compare as predições com a senoide original.\n",
    "3. Pergunta: A rede consegue manter a forma da senoide por um longo período de predição? O que acontece com as predições ao longo do tempo?\n",
    "\n",
    "### **Exercício 3: Adicionando Ruído**\n",
    "\n",
    "1. Adicione um ruído gaussiano mais forte à série temporal original (aumente a variância do ruído).\n",
    "2. Treine a RNN com essa série temporal ruidosa.\n",
    "3. Compare as predições com o modelo treinado com menos ruído.\n",
    "4. Pergunta: Como o aumento do ruído nos dados de entrada impacta a capacidade da RNN de prever corretamente a série temporal?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
