{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-55cb567b-4729-4c04-99f8-cffa3af12ab0-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Olá\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_joke = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
    "\n",
    "prompt = prompt_joke.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inteligência Artificial é o futuro!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates de Mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-ba55c9f4-0598-4798-9230-928e3cf83104-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "]\n",
    "\n",
    "# messages = [\n",
    "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
    "#     (\"human\", \"Hello, how are you?\"),\n",
    "# ]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_joke = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_joke.invoke({\n",
    "    \"lingua_origem\": \"inglês\",\n",
    "    \"lingua_destino\": \"português\",\n",
    "    \"texto\": \"Hello, how are you?\"\n",
    "})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, como você está?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "content='A capital do Rio Grande do Norte é Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-7bb92146-1851-4616-9f61-9826ba178f5c-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Saída do parser:\n",
      "A capital do Rio Grande do Norte é Natal.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
    "output = str_parser.invoke(response)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"Saída do parser:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "content='```json\\n{\\n  \"próton\": {\\n    \"massa\": \"1,67 x 10^-27 kg\",\\n    \"carga\": \"+1,6 x 10^-19 C\"\\n  },\\n  \"nêutron\": {\\n    \"massa\": \"1,675 x 10^-27 kg\",\\n    \"carga\": \"0 C\"\\n  },\\n  \"elétron\": {\\n    \"massa\": \"9,11 x 10^-31 kg\",\\n    \"carga\": \"-1,6 x 10^-19 C\"\\n  }\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 38, 'total_tokens': 158, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-4a9ab6f3-36bc-47b5-9d4c-7bfef25c759a-0' usage_metadata={'input_tokens': 38, 'output_tokens': 120, 'total_tokens': 158, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Saída do parser:\n",
      "{'próton': {'massa': '1,67 x 10^-27 kg', 'carga': '+1,6 x 10^-19 C'}, 'nêutron': {'massa': '1,675 x 10^-27 kg', 'carga': '0 C'}, 'elétron': {'massa': '9,11 x 10^-31 kg', 'carga': '-1,6 x 10^-19 C'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "response = llm.invoke(\"Quais as massas e cargas das partículas que constituem o átomo? Responda no formato JSON em que cada chave seja o nome da partícula\")\n",
    "output = json_parser.invoke(response)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"Saída do parser:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encadeamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_joke | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Las playas de Recife tienen tiburones!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"lingua_origem\": \"inglês\",\n",
    "    \"lingua_destino\": \"espanhol\",\n",
    "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texto, lingua_origem, lingua_destino):\n",
    "    response = chain.invoke({\n",
    "        \"lingua_origem\": lingua_origem,\n",
    "        \"lingua_destino\": lingua_destino,\n",
    "        \"texto\": texto\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Las playas de Recife tienen tiburones!\n"
     ]
    }
   ],
   "source": [
    "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Por que a sogra levou um lápis para a cama?\\n\\nPorque ela queria desenhar os sonhos da filha! 😂'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_joke = ChatPromptTemplate.from_template(\"Crie uma piada sobre: {topic}\")\n",
    "\n",
    "chain = prompt_joke | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"topic\": \"sogra\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_code = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Analise o texto e realize uma classificação de sentimento. O resultado deve ser: Positivo, Neutro ou Negativo.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt_code | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"texto\": \"Este almoço estava horrível. Nunca mais comerei essa comida.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3\n",
    "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def capitalize_string(input_string):\n",
      "    \"\"\"\n",
      "    Capitalizes the first letter of each word in a given string.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The string to be capitalized.\n",
      "\n",
      "    Returns:\n",
      "        str: The capitalized string.\n",
      "    \"\"\"\n",
      "    return input_string.title()\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    sample_text = \"hello world! this is a test.\"\n",
      "    capitalized_text = capitalize_string(sample_text)\n",
      "    print(capitalized_text)  # Output: \"Hello World! This Is A Test.\"\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_code = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"você é um programador senior python e irá gerar um bloco de código bem formatado. \\\n",
    "            gere apenas o bloco de código.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt_code | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"texto\": \"generate a code for capitalize a string\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4\n",
    "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solar system consists of the Sun and all the celestial bodies that orbit around it. This includes planets such as Earth, Mars, and Jupiter, their moons, asteroids, comets, and meteoroids. The Sun is a star and provides light and heat to the planets, making life on Earth possible. The solar system formed about 4.6 billion years ago from a cloud of gas and dust in space. It is part of the Milky Way, our galaxy.\n"
     ]
    }
   ],
   "source": [
    "prompt_explanation = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"De maneira simplificada explique o assunto\"),\n",
    "        (\"user\", \"{topico}\")\n",
    "    ]\n",
    ")\n",
    "prompt_translator = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é fluente em inglês britânico e irá traduzir textos\"),\n",
    "        (\"user\", \"{topico}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_explanation | llm | StrOutputParser() | prompt_translator | llm | StrOutputParser()\n",
    "print(chain.invoke({\"topico\": \"sistema solar\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5 - Desafio\n",
    "Crie uma `chain` que responda perguntas sobre o CESAR School."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Cesar School é uma escola de tecnologia e inovação que oferece programas de educação, com foco em formação prática e multidisciplinar. Ela é parte do grupo CESAR, uma organização reconhecida por seu ambiente inovador e desenvolvimento de soluções em tecnologia. Na Cesar School, você pode encontrar diversos cursos e especializações nas áreas de design, desenvolvimento de software, gestão de projetos e muito mais. O objetivo é preparar os alunos para os desafios do mercado de trabalho, com uma abordagem prática que envolve projetos reais e aprendizado colaborativo. Se tiver mais perguntas, estou aqui para ajudar!\n"
     ]
    }
   ],
   "source": [
    "prompt_cesar_school = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um bot de ajuda especializado na Cesar School.\\\n",
    "          Responda de maneira amigávem e solícita.\"),\n",
    "        (\"user\"), \"{question}\"\n",
    "    ]\n",
    ")\n",
    "chain = prompt_cesar_school | llm | StrOutputParser()\n",
    "print(chain.invoke({\"question\": \"O que é a Cesar School??\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
