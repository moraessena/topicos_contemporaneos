{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ol√°! Como posso ajudar voc√™ hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-55cb567b-4729-4c04-99f8-cffa3af12ab0-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Ol√°\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Traduza o seguinte texto para portugu√™s: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_joke = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portugu√™s: {texto}\")\n",
    "\n",
    "prompt = prompt_joke.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelig√™ncia Artificial √© o futuro!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates de Mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ol√°, como voc√™ est√°?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-ba55c9f4-0598-4798-9230-928e3cf83104-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "]\n",
    "\n",
    "# messages = [\n",
    "#     (\"system\", \"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
    "#     (\"human\", \"Hello, how are you?\"),\n",
    "# ]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_joke = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Voc√™ √© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_joke.invoke({\n",
    "    \"lingua_origem\": \"ingl√™s\",\n",
    "    \"lingua_destino\": \"portugu√™s\",\n",
    "    \"texto\": \"Hello, how are you?\"\n",
    "})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°, como voc√™ est√°?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "content='A capital do Rio Grande do Norte √© Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-7bb92146-1851-4616-9f61-9826ba178f5c-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Sa√≠da do parser:\n",
      "A capital do Rio Grande do Norte √© Natal.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
    "output = str_parser.invoke(response)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"Sa√≠da do parser:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "content='```json\\n{\\n  \"pr√≥ton\": {\\n    \"massa\": \"1,67 x 10^-27 kg\",\\n    \"carga\": \"+1,6 x 10^-19 C\"\\n  },\\n  \"n√™utron\": {\\n    \"massa\": \"1,675 x 10^-27 kg\",\\n    \"carga\": \"0 C\"\\n  },\\n  \"el√©tron\": {\\n    \"massa\": \"9,11 x 10^-31 kg\",\\n    \"carga\": \"-1,6 x 10^-19 C\"\\n  }\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 38, 'total_tokens': 158, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-4a9ab6f3-36bc-47b5-9d4c-7bfef25c759a-0' usage_metadata={'input_tokens': 38, 'output_tokens': 120, 'total_tokens': 158, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Sa√≠da do parser:\n",
      "{'pr√≥ton': {'massa': '1,67 x 10^-27 kg', 'carga': '+1,6 x 10^-19 C'}, 'n√™utron': {'massa': '1,675 x 10^-27 kg', 'carga': '0 C'}, 'el√©tron': {'massa': '9,11 x 10^-31 kg', 'carga': '-1,6 x 10^-19 C'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "response = llm.invoke(\"Quais as massas e cargas das part√≠culas que constituem o √°tomo? Responda no formato JSON em que cada chave seja o nome da part√≠cula\")\n",
    "output = json_parser.invoke(response)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"Sa√≠da do parser:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encadeamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_joke | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Las playas de Recife tienen tiburones!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"lingua_origem\": \"ingl√™s\",\n",
    "    \"lingua_destino\": \"espanhol\",\n",
    "    \"texto\": \"As praias de Recife tem tubar√µes!\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texto, lingua_origem, lingua_destino):\n",
    "    response = chain.invoke({\n",
    "        \"lingua_origem\": lingua_origem,\n",
    "        \"lingua_destino\": lingua_destino,\n",
    "        \"texto\": texto\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Las playas de Recife tienen tiburones!\n"
     ]
    }
   ],
   "source": [
    "output = translate(\"The beaches of Recife have sharks!\", \"ingl√™s\", \"espanhol\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exerc√≠cios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1\n",
    "Crie uma `chain` que a partir de um t√≥pico informado pelo usu√°rio, crie uma piada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Por que a sogra levou um l√°pis para a cama?\\n\\nPorque ela queria desenhar os sonhos da filha! üòÇ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_joke = ChatPromptTemplate.from_template(\"Crie uma piada sobre: {topic}\")\n",
    "\n",
    "chain = prompt_joke | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"topic\": \"sogra\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 2\n",
    "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negativo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_code = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Analise o texto e realize uma classifica√ß√£o de sentimento. O resultado deve ser: Positivo, Neutro ou Negativo.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt_code | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"texto\": \"Este almo√ßo estava horr√≠vel. Nunca mais comerei essa comida.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 3\n",
    "Crie uma `chain` que gere o c√≥digo de uma fun√ß√£o Python de acordo com a descri√ß√£o do usu√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def capitalize_string(input_string):\n",
      "    \"\"\"\n",
      "    Capitalizes the first letter of each word in a given string.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The string to be capitalized.\n",
      "\n",
      "    Returns:\n",
      "        str: The capitalized string.\n",
      "    \"\"\"\n",
      "    return input_string.title()\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    sample_text = \"hello world! this is a test.\"\n",
      "    capitalized_text = capitalize_string(sample_text)\n",
      "    print(capitalized_text)  # Output: \"Hello World! This Is A Test.\"\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_code = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"voc√™ √© um programador senior python e ir√° gerar um bloco de c√≥digo bem formatado. \\\n",
    "            gere apenas o bloco de c√≥digo.\"),\n",
    "        (\"user\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt_code | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"texto\": \"generate a code for capitalize a string\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 4\n",
    "Crie uma `chain` que explique de forma simplificada um t√≥pico geral fornecido pelo usu√°rio e, em seguida, traduza a explica√ß√£o para ingl√™s. Utilize dois templates encadeados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solar system consists of the Sun and all the celestial bodies that orbit around it. This includes planets such as Earth, Mars, and Jupiter, their moons, asteroids, comets, and meteoroids. The Sun is a star and provides light and heat to the planets, making life on Earth possible. The solar system formed about 4.6 billion years ago from a cloud of gas and dust in space. It is part of the Milky Way, our galaxy.\n"
     ]
    }
   ],
   "source": [
    "prompt_explanation = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"De maneira simplificada explique o assunto\"),\n",
    "        (\"user\", \"{topico}\")\n",
    "    ]\n",
    ")\n",
    "prompt_translator = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Voc√™ √© fluente em ingl√™s brit√¢nico e ir√° traduzir textos\"),\n",
    "        (\"user\", \"{topico}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_explanation | llm | StrOutputParser() | prompt_translator | llm | StrOutputParser()\n",
    "print(chain.invoke({\"topico\": \"sistema solar\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 5 - Desafio\n",
    "Crie uma `chain` que responda perguntas sobre o CESAR School."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Cesar School √© uma escola de tecnologia e inova√ß√£o que oferece programas de educa√ß√£o, com foco em forma√ß√£o pr√°tica e multidisciplinar. Ela √© parte do grupo CESAR, uma organiza√ß√£o reconhecida por seu ambiente inovador e desenvolvimento de solu√ß√µes em tecnologia. Na Cesar School, voc√™ pode encontrar diversos cursos e especializa√ß√µes nas √°reas de design, desenvolvimento de software, gest√£o de projetos e muito mais. O objetivo √© preparar os alunos para os desafios do mercado de trabalho, com uma abordagem pr√°tica que envolve projetos reais e aprendizado colaborativo. Se tiver mais perguntas, estou aqui para ajudar!\n"
     ]
    }
   ],
   "source": [
    "prompt_cesar_school = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Voc√™ √© um bot de ajuda especializado na Cesar School.\\\n",
    "          Responda de maneira amig√°vem e sol√≠cita.\"),\n",
    "        (\"user\"), \"{question}\"\n",
    "    ]\n",
    ")\n",
    "chain = prompt_cesar_school | llm | StrOutputParser()\n",
    "print(chain.invoke({\"question\": \"O que √© a Cesar School??\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
